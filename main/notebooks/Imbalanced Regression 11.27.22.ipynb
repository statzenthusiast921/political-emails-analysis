{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data from Github and set up for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19000, 20)\n",
      "(19000, 20)\n",
      "(19000, 20)\n",
      "(17148, 20)\n"
     ]
    }
   ],
   "source": [
    "#Read in the data\n",
    "import pandas as pd\n",
    "\n",
    "df1=pd.read_csv(\"https://raw.githubusercontent.com/statzenthusiast921/political-emails-analysis/main/main/data/clean_emails_df1.csv\")\n",
    "df2=pd.read_csv(\"https://raw.githubusercontent.com/statzenthusiast921/political-emails-analysis/main/main/data/clean_emails_df2.csv\")\n",
    "df3=pd.read_csv(\"https://raw.githubusercontent.com/statzenthusiast921/political-emails-analysis/main/main/data/clean_emails_df3.csv\")\n",
    "df4=pd.read_csv(\"https://raw.githubusercontent.com/statzenthusiast921/political-emails-analysis/main/main/data/clean_emails_df4.csv\")\n",
    "\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "print(df3.shape)\n",
    "print(df4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>party</th>\n",
       "      <th>country</th>\n",
       "      <th>locality</th>\n",
       "      <th>office</th>\n",
       "      <th>time</th>\n",
       "      <th>AM_PM</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Hour_Mil</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>month_num</th>\n",
       "      <th>cleaned_body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>compound</th>\n",
       "      <th>comp_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TAKE ACTION for Freedom: #CloseTheCamps</td>\n",
       "      <td>July 3, 2019</td>\n",
       "      <td>Dear friend, We have all seen the images and r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11:31 PM</td>\n",
       "      <td>PM</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>July</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>dear friend seen images read stories migrants ...</td>\n",
       "      <td>{'neg': 0.207, 'neu': 0.674, 'pos': 0.12, 'com...</td>\n",
       "      <td>-0.9807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trauma-informed schools</td>\n",
       "      <td>July 3, 2019</td>\n",
       "      <td>Team,Access to education is fundamental to a c...</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>United States</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>President of the United States</td>\n",
       "      <td>11:32 PM</td>\n",
       "      <td>PM</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>July</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>team access education fundamental child succes...</td>\n",
       "      <td>{'neg': 0.143, 'neu': 0.658, 'pos': 0.199, 'co...</td>\n",
       "      <td>0.7269</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   subject          date  \\\n",
       "1  TAKE ACTION for Freedom: #CloseTheCamps  July 3, 2019   \n",
       "2                  trauma-informed schools  July 3, 2019   \n",
       "\n",
       "                                                body       party  \\\n",
       "1  Dear friend, We have all seen the images and r...         NaN   \n",
       "2  Team,Access to education is fundamental to a c...  Democratic   \n",
       "\n",
       "         country locality                          office      time AM_PM  \\\n",
       "1  United States      NaN                             NaN  11:31 PM    PM   \n",
       "2  United States     Ohio  President of the United States  11:32 PM    PM   \n",
       "\n",
       "   Hour  Hour_Mil month  day  year  month_num  \\\n",
       "1    11        23  July    3  2019          7   \n",
       "2    11        23  July    3  2019          7   \n",
       "\n",
       "                                        cleaned_body  \\\n",
       "1  dear friend seen images read stories migrants ...   \n",
       "2  team access education fundamental child succes...   \n",
       "\n",
       "                                           sentiment  compound  comp_score  \n",
       "1  {'neg': 0.207, 'neu': 0.674, 'pos': 0.12, 'com...   -0.9807           0  \n",
       "2  {'neg': 0.143, 'neu': 0.658, 'pos': 0.199, 'co...    0.7269           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stack 4 dataframes together\n",
    "df = pd.concat([df1, df2, df3, df4], ignore_index=True, axis=0)\n",
    "#Delete first row and column\n",
    "df = df[1:]\n",
    "df = df.iloc[: , 1:]\n",
    "#Preview dataframe\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    61281\n",
       "0    12866\n",
       "Name: comp_score, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comp_score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51902,)\n",
      "(51902,)\n",
      "(22245,)\n",
      "(22245,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['comp_score']\n",
    "X = df['cleaned_body'].astype('U').values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: (51902, 302765)\n",
      "Test Data: (22245, 302765)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X)\n",
    "\n",
    "\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "print('Train Data:',X_train.shape)\n",
    "print('Test Data:',X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling to correct for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label 'Positive': 42907\n",
      "Before OverSampling, counts of label 'Negative': 8995 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label 'Positive': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before OverSampling, counts of label 'Negative': {} \\n\".format(sum(y_train == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OverSampling, the shape of X_train: (85814, 302765)\n",
      "After OverSampling, the shape of y_train: (85814,) \n",
      "\n",
      "After OverSampling, counts of label 'Positive': 42907\n",
      "After OverSampling, counts of label 'Negative': 42907\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel()) \n",
    "  \n",
    "print('After OverSampling, the shape of X_train: {}'.format(X_train_res.shape)) \n",
    "print('After OverSampling, the shape of y_train: {} \\n'.format(y_train_res.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label 'Positive': {}\".format(sum(y_train_res == 1))) \n",
    "print(\"After OverSampling, counts of label 'Negative': {}\".format(sum(y_train_res == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.82      0.78      3871\n",
      "    Positive       0.96      0.94      0.95     18374\n",
      "\n",
      "    accuracy                           0.92     22245\n",
      "   macro avg       0.86      0.88      0.87     22245\n",
      "weighted avg       0.92      0.92      0.92     22245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score, f1_score\n",
    "\n",
    "#Fit the model on oversampled data\n",
    "lr1 = LogisticRegression(max_iter=1000) \n",
    "lr1.fit(X_train_res, y_train_res.ravel()) \n",
    "predictions = lr1.predict(X_test) \n",
    "  \n",
    "#Print classification report \n",
    "print(classification_report(y_test, predictions,target_names=['Negative','Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18374\n",
       "0     3871\n",
       "Name: comp_score, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3162   709]\n",
      " [ 1030 17344]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling to correct for class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Undersampling, counts of label 'Positive': 42907\n",
      "Before Undersampling, counts of label 'Negative': 8995 \n",
      "\n",
      "After Undersampling, the shape of X_train: (17990, 302765)\n",
      "After Undersampling, the shape of y_train: (17990,) \n",
      "\n",
      "After Undersampling, counts of label 'Positive': 8995\n",
      "After Undersampling, counts of label 'Negative': 8995\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Undersampling, counts of label 'Positive': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before Undersampling, counts of label 'Negative': {} \\n\".format(sum(y_train == 0))) \n",
    "  \n",
    "# apply near miss \n",
    "from imblearn.under_sampling import NearMiss \n",
    "nr = NearMiss() \n",
    "  \n",
    "X_train_miss, y_train_miss = nr.fit_resample(X_train, y_train.ravel()) \n",
    "  \n",
    "print('After Undersampling, the shape of X_train: {}'.format(X_train_miss.shape)) \n",
    "print('After Undersampling, the shape of y_train: {} \\n'.format(y_train_miss.shape)) \n",
    "  \n",
    "print(\"After Undersampling, counts of label 'Positive': {}\".format(sum(y_train_miss == 1))) \n",
    "print(\"After Undersampling, counts of label 'Negative': {}\".format(sum(y_train_miss == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.40      0.94      0.56      3871\n",
      "    Positive       0.98      0.71      0.82     18374\n",
      "\n",
      "    accuracy                           0.75     22245\n",
      "   macro avg       0.69      0.82      0.69     22245\n",
      "weighted avg       0.88      0.75      0.78     22245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the model on train set \n",
    "lr2 = LogisticRegression(max_iter=1000) \n",
    "lr2.fit(X_train_miss, y_train_miss.ravel()) \n",
    "predictions = lr2.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(classification_report(y_test, predictions,target_names=['Negative','Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3622   249]\n",
      " [ 5404 12970]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "email_env",
   "language": "python",
   "name": "email_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
